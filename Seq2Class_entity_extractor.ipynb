{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrator de entidades - sequence to class - BiLSTM  e Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for training neural model with embedding\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omni.nlp import TextProcessing\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Input, LSTM, TimeDistributed, Bidirectional\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import json\n",
    "import string\n",
    "from itertools import compress\n",
    "from tensorflow.python.keras.models import Model as ModelKeras\n",
    "\n",
    "from mlflow import log_metric, log_param, log_artifact, set_tracking_uri, set_experiment, start_run\n",
    "import mlflow.tensorflow\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "keys_translator = list(translator.keys())\n",
    "key_to_change = [x == '-' for x in string.punctuation]\n",
    "for key in list(compress(keys_translator, key_to_change)):\n",
    "    translator[key] = \" \"\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow as tf\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "#sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras.utils import CustomObjectScope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "#Functions\n",
    "\n",
    "def text_preprocessing(txt):\n",
    "    nlp = TextProcessing()\n",
    "    txt = nlp.preprocess_str(txt=txt, clean_html=False, explode_digits=False)\n",
    "    #txt = nlp.preprocess_str(txt=txt, clean_html=False)\n",
    "    txt = txt.translate(translator)\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def convert_tokens_to_charvec_timeframes(dataframe=None, column_selected=None, embedding_model=None,\n",
    "                                         char_max_length=None, max_length=None, char_to_int_dict=None):\n",
    "\n",
    "    # Preparando o conjunto de dados de entrada com timestamps (vetores e indices)\n",
    "    vectors_dim = 256\n",
    "\n",
    "    dataX_vec = np.zeros([dataframe.shape[0], max_length, vectors_dim], dtype='float16')\n",
    "\n",
    "    print(dataX_vec.shape)\n",
    "    \n",
    "    i = 0\n",
    "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "        seq = 0\n",
    "        txt = str(row[column_selected])\n",
    "        tokens = list(txt.split(\" \"))\n",
    "        for item in tokens:\n",
    "            if seq < max_length:\n",
    "                # Evaluate a word\n",
    "                word_vec = get_char_embedding_from_word(word=item, model_emb=embedding_model, char_to_int_dict=char_to_int_dict,\n",
    "                                                        max_length=char_max_length)\n",
    "                dataX_vec[i, seq, :] = word_vec\n",
    "            else:\n",
    "                break\n",
    "            seq = seq + 1\n",
    "        i = i + 1\n",
    "\n",
    "    return dataX_vec\n",
    "\n",
    "def get_char_embedding_from_word(word, model_emb, char_to_int_dict, max_length):\n",
    "    dim = len(char_to_int_dict)\n",
    "    encoded_data = np.zeros((1, max_length, dim), dtype='int')\n",
    "\n",
    "    seq = 0\n",
    "    for char in word:\n",
    "        if seq < max_length:\n",
    "            encoded_data[0, seq, char_to_int_dict[char]] = 1\n",
    "            seq = seq + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # implementacao para uma entrada de seq\n",
    "    states_value_seq = model_emb.predict(encoded_data)\n",
    "    embedded_samples = states_value_seq[0][0]\n",
    "\n",
    "    return embedded_samples\n",
    "\n",
    "\n",
    "def convert_tokens_to_vec_timeframes_w2v(dataframe=None, column_selected=None, embedding_model=None, embedding_model_code=None,\n",
    "                                         max_length=None, verbose=None):\n",
    "    nlp = TextProcessing()\n",
    "    if embedding_model_code == 'gensim_word2vec' or embedding_model_code == 'gensim_fasttext':\n",
    "\n",
    "        # Preparando o conjunto de dados de entrada com timestamps (vetores e indices)\n",
    "        vectors_dim = embedding_model.wv.vector_size\n",
    "\n",
    "        dataX_vec = np.zeros([dataframe.shape[0], max_length, vectors_dim], dtype='float16')\n",
    "\n",
    "        # Transforming txt lists to lists of lists\n",
    "        nlp = TextProcessing()\n",
    "        list_tokens_input = nlp.convert_txt_of_tokens_to_list_of_tokens_from_pandas(\n",
    "            dataframe=dataframe,\n",
    "            column_selected=column_selected)\n",
    "\n",
    "        # Qtde de palavras a serem processadas\n",
    "        list_tokens_flat = nlp.flat_list_2D(list_tokens_input)\n",
    "        qtde_tokens_input = len(list_tokens_flat)\n",
    "        qtde_tokens_dist = len(set(list_tokens_flat))\n",
    "\n",
    "        # Qtde de palavras mapeadas no espaco\n",
    "        qtde_tokens_espaco = len(embedding_model.wv.vocab)\n",
    "\n",
    "        if verbose > 1:\n",
    "            logging.info('Total of distinct tokens from dataset input : ' + str(qtde_tokens_dist))\n",
    "\n",
    "        # iniciando contador de amostras\n",
    "        index_sample = 0\n",
    "\n",
    "        # iniciando o vetor de tamanhos de sentencas\n",
    "        samples_len = np.zeros(len(list_tokens_input), dtype='int16')\n",
    "        tokens_not_found = np.zeros(len(list_tokens_input), dtype='int16')\n",
    "        tokens_not_found_txt = []\n",
    "        samples_tokens_cover = np.zeros(len(list_tokens_input), dtype='float16')\n",
    "\n",
    "        # para cada sentenca na matriz de entrada\n",
    "        for line in tqdm(list_tokens_input):\n",
    "            index_token = 0\n",
    "            samples_len[index_sample] = len(line)\n",
    "            # para cada token da sentenca de entrada\n",
    "            for token in line:\n",
    "                if index_token < max_length:\n",
    "                    if token in embedding_model.wv.vocab:\n",
    "                        # se o token existe no wv podemos validar um timeframe valido\n",
    "                        dataX_vec[index_sample, index_token, :] = embedding_model.wv[token]\n",
    "                    else:\n",
    "                        # o token nao existe no espaco semantico - nao avancar no timeframe\n",
    "                        tokens_not_found[index_sample] = tokens_not_found[index_sample] + 1\n",
    "\n",
    "                        # armazenando lista de tokens nao encontrados\n",
    "                        tokens_not_found_txt.append(token)\n",
    "\n",
    "                        # qdo token nao é mapeado nao se considera no vetor de entrada (retorno do contador se nao estourou o limite)\n",
    "                        if tokens_not_found[index_sample] < max_length:\n",
    "                            index_token = index_token - 1\n",
    "                        else:\n",
    "                            index_token = max_length\n",
    "                            if verbose > 1:\n",
    "                                logging.error(\n",
    "                                    'Not found embedding tokens enough to build this sample input: ' + str(line))\n",
    "\n",
    "                index_token += 1\n",
    "\n",
    "            # contabilizando cobertura de tokens\n",
    "            samples_tokens_cover[index_sample] = (samples_len[index_sample] - tokens_not_found[index_sample]) / \\\n",
    "                                                 samples_len[index_sample]\n",
    "\n",
    "            index_sample += 1\n",
    "\n",
    "        # todo fazer revisao dos tokens e apresentar so um consolidado\n",
    "\n",
    "        if verbose > 1:\n",
    "            logging.info('Mean size of txt input (tokens): ' + str(samples_len.mean()))\n",
    "            logging.info('Min size of txt input (tokens): ' + str(samples_len.min()))\n",
    "            logging.info('Max size of txt input (tokens): ' + str(samples_len.max()))\n",
    "\n",
    "            # Calculando cobertura do espaco semantico\n",
    "            logging.info('Total of processed tokens: ' + str(qtde_tokens_input))\n",
    "            logging.info('Total of not embedded tokens: ' + str(tokens_not_found.sum()))\n",
    "            logging.info('Tokens volume embedding covering: ' + str(\n",
    "                (qtde_tokens_input - tokens_not_found.sum()) / (qtde_tokens_input)))\n",
    "\n",
    "        # Cobertura em variedade de tokens\n",
    "        perc_cobertura = ((qtde_tokens_espaco) - (len(set(tokens_not_found_txt)))) / (qtde_tokens_espaco)\n",
    "\n",
    "        if verbose > 1:\n",
    "            logging.info('Tokens variety embedding covering: ' + str(perc_cobertura))\n",
    "\n",
    "        # Padronizando a entrada de dados com uma dimensionalidade constante\n",
    "        X_pad_vec = pad_sequences(dataX_vec, maxlen=max_length, dtype='float16', padding='post', truncating='post',\n",
    "                                  value=0.0)\n",
    "\n",
    "        return X_pad_vec, samples_tokens_cover\n",
    "    else:\n",
    "        raise (Exception('Invalid embedding model code selection: ' + embedding_model_code))\n",
    "        return False\n",
    "\n",
    "def plot_attention_weights2(encoder_inputs, attention_weights, classe):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
    "              '\\nPlease check if the decoder produced  a proper translation')\n",
    "        return\n",
    "\n",
    "    attention_mat = attention_weights[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels(classe)\n",
    "    ax.set_yticklabels([inp if inp != 0 else \"<Res>\" for inp in encoder_inputs.split(' ')])\n",
    "\n",
    "        \n",
    "def convert_tokens_to_charvec_timeframes_text(txt=None, embedding_model=None,\n",
    "                                         char_max_length=None, max_length=None, char_to_int_dict=None):\n",
    "    # Preparando o conjunto de dados de entrada com timestamps (vetores e indices)\n",
    "    vectors_dim = 256\n",
    "    \n",
    "    dataX_vec = np.zeros([1, max_length, vectors_dim], dtype='float16')\n",
    "    \n",
    "    tokens = list(txt.split(\" \"))\n",
    "    seq = 0\n",
    "    for item in tokens:\n",
    "        if seq < max_length:\n",
    "            # Evaluate a word\n",
    "            word_vec = get_char_embedding_from_word(word=item, model_emb=embedding_model, char_to_int_dict=char_to_int_dict,\n",
    "                                                    max_length=char_max_length)\n",
    "            dataX_vec[0, seq, :] = word_vec\n",
    "        else:\n",
    "                break\n",
    "        seq = seq + 1\n",
    "    return dataX_vec\n",
    "\n",
    "def convert_tokens_to_w2v_timeframes_text(txt=None, column_selected=None, embedding_model=None,\n",
    "                                         char_max_length=None, max_length=None, char_to_int_dict=None):\n",
    "    # Preparando o conjunto de dados de entrada com timestamps (vetores e indices)\n",
    "    vectors_dim = embedding_model.wv.vector_size\n",
    "\n",
    "    dataX_vec = np.zeros([1, max_length, vectors_dim], dtype='float16')\n",
    "    \n",
    "    tokens = list(txt.split(\" \"))\n",
    "    seq = 0\n",
    "    for item in tokens:\n",
    "        if seq < max_length:\n",
    "            # Evaluate a word\n",
    "            if item in embedding_model.wv.vocab:\n",
    "                # se o token existe no wv podemos validar um timeframe valido\n",
    "                dataX_vec[0, seq, :] = embedding_model.wv[item]\n",
    "                seq = seq + 1\n",
    "        \n",
    "    return dataX_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a integer dictionary from output text class\n",
    "def build_integer_class_dictionary_from_pandas(df_data, column_selected):\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    #collecting distinct tokens\n",
    "    for index, row in df_data.iterrows():\n",
    "        txt = str(row[column_selected])\n",
    "\n",
    "        if txt not in classes:\n",
    "            classes.append(txt)\n",
    "\n",
    "    classes = sorted(list(classes))\n",
    "\n",
    "    #reporting\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    print('Number of unique classes:', num_classes)\n",
    "\n",
    "    dict_int2txt = dict(\n",
    "        [(i, tk) for i, tk in enumerate(classes)])\n",
    "    dict_txt2int = dict(\n",
    "        [(tk, i) for i, tk in enumerate(classes)])\n",
    "\n",
    "    return dict_int2txt, dict_txt2int\n",
    "\n",
    "#create one-hot encode from txt and integer dictionary\n",
    "def build_one_hot_encoding_from_classes_target(df_data, column_selected, dict_txt2int):\n",
    "\n",
    "    n_samples = df_data[column_selected].shape[0]\n",
    "    dim = len(dict_txt2int)\n",
    "    encoded_target_data = np.zeros((n_samples, dim), dtype='int')\n",
    "\n",
    "    # assign 1's\n",
    "    sample = 0\n",
    "    for index, row in df_data.iterrows():\n",
    "\n",
    "        txt = str(row[column_selected])\n",
    "        encoded_target_data[sample, dict_txt2int[txt]] = 1\n",
    "        sample = sample + 1\n",
    "\n",
    "    return encoded_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_sample_maker(X, y, sample_size, random_seed=None):\n",
    "    \"\"\" return a balanced data set by sampling all classes with sample_size \n",
    "        current version is developed on assumption that the positive\n",
    "        class is the minority.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    X: {numpy.ndarrray}\n",
    "    y: {numpy.ndarray}\n",
    "    \"\"\"\n",
    "    uniq_levels = np.unique(y)\n",
    "    uniq_counts = Counter(y)\n",
    "\n",
    "    if not random_seed is None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # find observation index of each class levels\n",
    "    groupby_levels = {}\n",
    "    for ii, level in enumerate(uniq_levels):\n",
    "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "        groupby_levels[level] = obs_idx\n",
    "    # oversampling on observations of each label\n",
    "    balanced_copy_idx = []\n",
    "    for gb_level, gb_idx in groupby_levels.items():\n",
    "        over_sample_idx = np.random.choice(gb_idx, size=sample_size, replace=True).tolist()\n",
    "        balanced_copy_idx+=over_sample_idx\n",
    "    np.random.shuffle(balanced_copy_idx)\n",
    "\n",
    "    return balanced_copy_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_substantive_sequence_txt(input_text):\n",
    "    \n",
    "    input_seq1 = convert_tokens_to_charvec_timeframes_text(txt=input_text, embedding_model=wordchar2vec,\n",
    "                                         char_max_length=max_char_lenght, max_length=max_length, char_to_int_dict=char_to_int_dict)\n",
    "    input_seq2 = convert_tokens_to_w2v_timeframes_text(txt=input_text, embedding_model=word2vec,\n",
    "                                         char_max_length=max_char_lenght, max_length=max_length, char_to_int_dict=char_to_int_dict)\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    encoder_char_outputs, char_last_forward_h1, char_last_forward_c1, char_last_backward_h1, char_last_backward_c1 = encoder_char_model.predict(input_seq1)\n",
    "    encoder_w2v_outputs, w2v_last_forward_h1, w2v_last_forward_c1, w2v_last_backward_h1, w2v_last_backward_c1 = encoder_w2v_model.predict(input_seq2)\n",
    "\n",
    "    char_state_h = np.concatenate((char_last_forward_h1, char_last_backward_h1), axis=1)\n",
    "    char_state_c = np.concatenate((char_last_forward_c1, char_last_backward_c1), axis=1)\n",
    "    \n",
    "    w2v_state_h = np.concatenate((w2v_last_forward_h1, w2v_last_backward_h1), axis=1)\n",
    "    w2v_state_c = np.concatenate((w2v_last_forward_c1, w2v_last_backward_c1), axis=1)\n",
    "    \n",
    "    enc_state_h = np.concatenate((char_state_h, w2v_state_h), axis=1)\n",
    "    #print(states_value_h.shape)\n",
    "    enc_outs = np.concatenate((encoder_char_outputs, encoder_w2v_outputs), axis=2)\n",
    "\n",
    "    \n",
    "    decoder_inf_pred, attention_weights = classifier_model.predict([enc_outs, enc_state_h])\n",
    "    \n",
    "    key = np.argmax(decoder_inf_pred)\n",
    "    \n",
    "    return int_to_label_dict[key], attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights2(encoder_inputs, attention_weights, classe):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
    "              '\\nPlease check if the decoder produced  a proper translation')\n",
    "        return\n",
    "\n",
    "    attention_mat = attention_weights[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels(classe)\n",
    "    ax.set_yticklabels([inp if inp != 0 else \"<Res>\" for inp in encoder_inputs.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python import keras\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, names, labels, batch_size=50, dim_char=(30, 256), dim_w2v=(30, 100),\n",
    "                 n_classes=None, shuffle=True, wordchar2vec=None, word2vec=None, max_char_lenght=10,\n",
    "                 max_length=30 , char_to_int_dict=None):\n",
    "        'Initialization'\n",
    "        self.dim_char = dim_char\n",
    "        self.dim_w2v = dim_w2v\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.names = names\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.wordchar2vec = wordchar2vec\n",
    "        self.wordchar2vec._make_predict_function()\n",
    "        self.word2vec = word2vec\n",
    "        self.max_char_lenght = max_char_lenght\n",
    "        self.max_length = max_length\n",
    "        self.char_to_int_dict = char_to_int_dict\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        [X_char, X_w2v], y = self.__data_generation(list_IDs_temp)\n",
    "        #X_char, y = self.__data_generation(list_IDs_temp)\n",
    "        return [X_char, X_w2v], y\n",
    "        #return X_char, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X_char = np.empty((self.batch_size, *self.dim_char))\n",
    "        X_w2v = np.empty((self.batch_size, *self.dim_w2v))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            \n",
    "            X_char[i,] = convert_tokens_to_charvec_timeframes_text(txt = self.names[ID], embedding_model = self.wordchar2vec,\n",
    "                                         char_max_length = self.max_char_lenght, max_length = self.max_length, char_to_int_dict = self.char_to_int_dict)\n",
    "            \n",
    "            X_w2v[i,] = convert_tokens_to_w2v_timeframes_text(txt = self.names[ID], embedding_model = self.word2vec,\n",
    "                                         char_max_length = self.max_char_lenght, max_length = self.max_length, char_to_int_dict = self.char_to_int_dict)\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return [X_char, X_w2v], keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "        #return X_char, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention_Bahdanau(tf.keras.Model):\n",
    "class Attention_Bahdanau(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Attention_Bahdanau, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_with_time_axis = tf.expand_dims(inputs[1], 1)\n",
    "        score = tf.nn.tanh(self.W1(inputs[0]) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * inputs[0]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Attention_Bahdanau, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar e processar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "#Parameters\n",
    "data_file_path = \"data/labeled_offers_utilidades_domesticas_with_image.csv\"\n",
    "input_column = \"name\"\n",
    "output_column = \"substantive\"\n",
    "max_length = 30\n",
    "\n",
    "char_embedding_model_file = \"models/char_words_model_nd_256_es.h5\"\n",
    "input_w2v_file = \"models/word-2-vectors-1000.bin\"\n",
    "char_to_int_dict_file = \"models/char_to_int.json\"\n",
    "max_char_lenght = 10\n",
    "\n",
    "n_hidden_nodes = 300\n",
    "n_epochs = 100\n",
    "n_batch = 50\n",
    "\n",
    "log_dir_tensorboard = 'logs/seq2class-char-w2v-attention-substantive-30042020'\n",
    "filepath = 's2cls_substantive_extract_char_w2v_attention_00.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_experiment(\"/Users/andre.oliveira@omnilogic.com.br/seq2class_entity_extract_attention_char_w2v\")\n",
    "#run_name = \"seq2class_entity_extract_attention_char_w2v_v00\"\n",
    "#start_run(run_name=run_name)\n",
    "#log_param('n_hidden_nodes', n_hidden_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "#Load data\n",
    "df_dataset = pd.read_csv(data_file_path, error_bad_lines=False)\n",
    "\n",
    "df_dataset_f = df_dataset[df_dataset[output_column] != '-']\n",
    "\n",
    "df_dataset_f = df_dataset_f[df_dataset_f[input_column] != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retirar valores NAN do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop = []\n",
    "for x in range(df_dataset_f.shape[0]):\n",
    "    if type(df_dataset_f.name.values[x]) != str:\n",
    "        print(df_dataset_f.name.values[x])\n",
    "        list_to_drop.append(df_dataset_f.index[x])\n",
    "    if type(df_dataset_f.substantive.values[x]) != str:\n",
    "        print(df_dataset_f.substantive.values[x])\n",
    "        list_to_drop.append(df_dataset_f.index[x])\n",
    "\n",
    "df_dataset_f = df_dataset_f.drop(list_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade de ofertas por entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_substantive_offer_count = Counter(df_dataset_f.substantive.values.tolist())\n",
    "dict_substantive_offer_count = {k: v for k, v in sorted(dict_substantive_offer_count.items(), key=lambda item: item[1], reverse=True)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_substantive_offer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(dict_substantive_offer_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_substantives = list(dict_substantive_offer_count.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(all_substantives)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-Processamento dos camposde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing txt\n",
    "df_dataset_f[input_column] = df_dataset_f[input_column].swifter.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanceamento das classes pela lista de IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_ids = df_dataset_f.ID.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_substantives = df_dataset_f.substantive.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_substantives = list(set(list_all_substantives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_substantives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_copy_idx = balanced_sample_maker(list_all_ids, list_all_substantives, sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_balanced_ids = [list_all_ids[index] for index in balanced_copy_idx]\n",
    "list_balanced_substantives = [list_all_substantives[index] for index in balanced_copy_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list_balanced_substantives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicionário de conversão da classe de entidade para inteiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_label_dict , label_to_int_dict  = build_integer_class_dictionary_from_pandas(df_data=df_dataset_f,column_selected=output_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar arquivo json de dicionário pois o modelo retorna o valor inteiro da classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_label_dict_file = \"models/int_to_label_brand.json\"\n",
    "\n",
    "with open(int_to_label_dict_file, \"wb\") as f:\n",
    "    f.write(json.dumps(int_to_label_dict).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicionário com os valores dos campos para o Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_name = {}\n",
    "dict_id_substantive = {}\n",
    "\n",
    "for ind, row in tqdm(df_dataset_f.iterrows(), total = df_dataset_f.shape[0]):\n",
    "    dict_id_name[df_dataset_f.ID[ind]] = df_dataset_f.name[ind]\n",
    "    dict_id_substantive[df_dataset_f.ID[ind]] = label_to_int_dict[df_dataset_f.substantive[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input char embedding\n",
    "\n",
    "model_char = load_model(char_embedding_model_file, compile=False)\n",
    "\n",
    "encoder_inputs = model_char.input[0] #input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model_char.layers[2].output # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = ModelKeras(encoder_inputs, encoder_states)\n",
    "wordchar2vec = encoder_model\n",
    "\n",
    "with open(char_to_int_dict_file) as json_file:\n",
    "    char_to_int_dict = json.load(json_file)\n",
    "\n",
    "#load input word2vec embedding\n",
    "word2vec = Word2Vec.load(input_w2v_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'dim_char': (30, 256),\n",
    "          'dim_w2v': (30, 100),\n",
    "          'batch_size': 200,\n",
    "          'n_classes': len(label_to_int_dict.keys()),\n",
    "          'shuffle': True, \n",
    "          'wordchar2vec': wordchar2vec,\n",
    "          'word2vec': word2vec,\n",
    "          'max_char_lenght': max_char_lenght,\n",
    "          'max_length': max_length,\n",
    "          'char_to_int_dict': char_to_int_dict\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão entre conjunto de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_val, data_test = train_test_split(list_balanced_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = train_test_split(data_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(data_train, dict_id_name, dict_id_substantive, **params)\n",
    "validation_generator = DataGenerator(data_val, dict_id_name, dict_id_substantive, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_inputs = Input(shape=(params['dim_char'][0], params['dim_char'][1]))\n",
    "encoder_char = Bidirectional(LSTM(n_hidden_nodes, return_state=True, return_sequences=True, dropout=0.1))\n",
    "encoder_char_outputs, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_char(encoder_char_inputs)\n",
    "\n",
    "state_h1 = Concatenate()([forward_h1, backward_h1])\n",
    "state_c1 = Concatenate()([forward_c1, backward_c1])\n",
    "encoder_char_states = [state_h1, state_c1]\n",
    "\n",
    "encoder_w2v_inputs = Input(shape=(params['dim_w2v'][0], params['dim_w2v'][1]))\n",
    "encoder_w2v = Bidirectional(LSTM(n_hidden_nodes, return_state=True, return_sequences=True, dropout=0.1))\n",
    "encoder_w2v_outputs, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_w2v(encoder_w2v_inputs)\n",
    "\n",
    "state_h2 = Concatenate()([forward_h2, backward_h2])\n",
    "state_c2 = Concatenate()([forward_c2, backward_c2])\n",
    "encoder_w2v_states = [state_h2, state_c2]\n",
    "\n",
    "encoder_states_h  = Concatenate()([state_h1, state_h2])\n",
    "encoder_states_c  = Concatenate()([state_c1, state_c2])\n",
    "\n",
    "encoder_outputs = Concatenate()([encoder_char_outputs, encoder_w2v_outputs])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention_Bahdanau(units =4* n_hidden_nodes)\n",
    "context_vector, attention_weights = attn_layer([encoder_outputs, encoder_states_h])\n",
    "\n",
    "decoder_dense = Dense(units=params['n_classes'], activation='softmax')\n",
    "\n",
    "decoder_pred = decoder_dense(context_vector)\n",
    "\n",
    "model = ModelKeras([encoder_char_inputs, encoder_w2v_inputs], decoder_pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=0.001, verbose=1,\n",
    "                                  mode='auto',\n",
    "                                  cooldown=10)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1,\n",
    "                                  mode='auto')\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "tensorb = TensorBoard(log_dir= log_dir_tensorboard)\n",
    "\n",
    "callbacks_list = [reduce_lr, tensorb, checkpoint]\n",
    "\n",
    "# Enable auto-logging to MLflow to capture TensorBoard metrics.\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "\n",
    "model.fit_generator(epochs=n_epochs, verbose=1,\n",
    "                    generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=5,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 's2cls_substantive_extract_char_w2v_attention_00.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({'Attention_Bahdanau': Attention_Bahdanau}):\n",
    "    model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_inputs = model.input[0] #input_1\n",
    "encoder_char = model.layers[2]\n",
    "encoder_char_outputs, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_char(encoder_char_inputs)\n",
    "\n",
    "encoder_char_model = ModelKeras(inputs = [encoder_char_inputs], outputs =[encoder_char_outputs, forward_h1, forward_c1, backward_h1, backward_c1])\n",
    "\n",
    "encoder_w2v_inputs = model.input[1] #input_2\n",
    "encoder_w2v = model.layers[3]\n",
    "encoder_w2v_outputs, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_w2v(encoder_w2v_inputs)\n",
    "\n",
    "encoder_w2v_model = ModelKeras(inputs = [encoder_w2v_inputs], outputs =[encoder_w2v_outputs, forward_h2, forward_c2, backward_h2, backward_c2])\n",
    "\n",
    "encoder_inf_outputs = Input(shape=(max_length, 4*n_hidden_nodes,), name='encoder_inf_output')\n",
    "encoder_inf_state_h = Input(shape=(4*n_hidden_nodes,), name='encoder_inf_states')\n",
    "attn_layer = model.layers[8]\n",
    "context_vector, attention_weights = attn_layer([encoder_inf_outputs, encoder_inf_state_h])\n",
    "\n",
    "decoder_dense = model.layers[9]\n",
    "decoder_inf_pred=decoder_dense(context_vector)\n",
    "\n",
    "classifier_model = ModelKeras(inputs=[encoder_inf_outputs, encoder_inf_state_h],\n",
    "                           outputs=[decoder_inf_pred, attention_weights])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de Treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [dict_id_name[_id] for _id in data_train_val]\n",
    "target_substantive = [dict_id_substantive[_id] for _id in data_train_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_substantive = [int_to_label_dict[ind] for ind in target_substantive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_substantive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index = 45\n",
    "classe, attention_weights = classifier_substantive_sequence_txt(input_texts[seq_index])\n",
    "classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights2(input_texts[seq_index], attention_weights, classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(300):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    classe, attention_weights = classifier_substantive_sequence_txt(input_texts[seq_index])\n",
    "    print(f'-{seq_index}')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Target:', target_substantive[seq_index])\n",
    "    print('Predict:', classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in tqdm(range(len(input_texts))):\n",
    "    classe, attention_weights = classifier_substantive_sequence_txt(input_texts[seq_index])\n",
    "    predict_train.append(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy_score(target_substantive, predict_train)\n",
    "mlflow.log_metric('train_acc', train_acc)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts_test = [dict_id_name[_id] for _id in data_test]\n",
    "target_substantive_test = [dict_id_substantive[_id] for _id in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(300):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    classe, attention_weights = classifier_substantive_sequence_txt(input_texts_test[seq_index])\n",
    "    print(f'-{seq_index}')\n",
    "    print('Input sentence:', input_texts_test[seq_index])\n",
    "    print('Target:', int_to_label_dict[target_substantive_test[seq_index]])\n",
    "    print('Predict:', classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in tqdm(range(len(input_texts_test))):\n",
    "    classe, attention_weights = classifier_substantive_sequence_txt(input_texts_test[seq_index])\n",
    "    predict_test.append(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy_score(target_substantive_test, predict_test)\n",
    "mlflow.log_metric('test_acc', test_acc)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmbienteTeste",
   "language": "python",
   "name": "ambienteteste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
